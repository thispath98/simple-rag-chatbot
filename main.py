import os
import json
from typing import Optional, Dict, Any

import streamlit as st
from dotenv import load_dotenv

from utils.chat_util import (
    display_chat_history,
    add_to_chat_history,
    get_retrieved_docs,
    get_input_message,
    stream_data,
)
from utils.llms import RouterAgent, RerankerAgent, ResponderAgent
from utils.vectorstore_util import get_milvus_client, semantic_search


def initialize_session_state() -> None:
    """
    Initialize Streamlit session state variables.

    This function sets up the initial state for chat history and OpenAI configuration.
    """
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "OPENAI_API_KEY" not in st.session_state:
        st.session_state.OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
    if "OPENAI_MODEL" not in st.session_state:
        st.session_state.OPENAI_MODEL = os.environ["OPENAI_MODEL"]


def process_rag_query(
    client: Any,
    reranker: RerankerAgent,
    responder: ResponderAgent,
    user_query: str,
    input_message: str,
) -> str:
    """
    Process a query that requires RAG (Retrieval Augmented Generation).

    Args:
        client: Milvus client instance
        reranker: Reranker agent instance
        responder: Responder agent instance
        user_query: User's question
        input_message: Formatted chat history

    Returns:
        str: Generated response
    """
    # Step 1: Vector search
    with st.spinner("ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        search_result = semantic_search(client, user_query)

    # Step 2: Rerank documents
    with st.spinner("ê´€ë ¨ ë¬¸ì„œë¥¼ ì •ë ¬ ì¤‘ì…ë‹ˆë‹¤..."):
        retrieved_query = [e["entity"]["query_text"] for e in search_result[0]]
        rank_indices = reranker.rank_documents(user_query, retrieved_query)

    # Step 3: Format retrieved documents
    with st.spinner("ë‹µë³€ì— ì‚¬ìš©í•  ë¬¸ì„œë¥¼ ì •ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
        retrieved_docs = get_retrieved_docs(search_result, rank_indices)

    # Step 4: Generate response
    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        response = responder.generate_response(
            input_message,
            retrieved_docs,
            user_query,
        )

    return response


# Configure page
st.set_page_config(page_title="ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ AI FAQ ì±—ë´‡", page_icon="ğŸ›ï¸")
st.title("ğŸ›ï¸ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ AI FAQ ì±—ë´‡")

# Load environment variables and initialize session state
load_dotenv(override=True)
initialize_session_state()

# Display chat history
display_chat_history(st.session_state.chat_history)

# Initialize agents and client
client = get_milvus_client()
router = RouterAgent(model=st.session_state.OPENAI_MODEL)
reranker = RerankerAgent(model=st.session_state.OPENAI_MODEL)
responder = ResponderAgent(model=st.session_state.OPENAI_MODEL)

# Handle user input
if user_query := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    try:
        # Display user message
        with st.chat_message("user"):
            st.markdown(user_query)

            # Add current query to the history
            add_to_chat_history(role="user", content=user_query)

        # get input message
        input_message = get_input_message(st.session_state.chat_history)

        # Route the query
        with st.spinner("ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ë‹µë³€ ë°©ì‹ì„ íŒë‹¨ ì¤‘ì…ë‹ˆë‹¤..."):
            tool_call = router.route_answer(input_message, user_query)
        name = tool_call.name
        args = json.loads(tool_call.arguments)

        if name == "not_relevant":
            response = "ì €ëŠ” ìŠ¤ë§ˆíŠ¸ ìŠ¤í† ì–´ FAQë¥¼ ìœ„í•œ ì±—ë´‡ì…ë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ ìŠ¤í† ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."

            # Remove current query from the history
            st.session_state.chat_history.pop()

        elif name == "rag_required":
            user_query = args["query"]
            response = process_rag_query(
                client, reranker, responder, user_query, input_message
            )

            # Save to chat history
            add_to_chat_history(role="assistant", content=response)

        # Display assistant response
        with st.chat_message("assistant"):
            st.write_stream(stream_data(response))

    except Exception as e:
        st.error(e)
